{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение контеста Dota 2: Win Probability Prediction \n",
    "### Курс \"Машинное обучение\" ММП ВМК МГУ\n",
    "### Каюмов Эмиль"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: предсказать по первым пяти минутам матча команду-победителя.\n",
    "\n",
    "Метрика: AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовительные работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nolearn.lasagne import NeuralNet, TrainSplit\n",
    "from lasagne.layers import DenseLayer, InputLayer, DropoutLayer\n",
    "from lasagne.updates import adagrad\n",
    "from lasagne.nonlinearities import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем базовый датасет и выделим target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('input/features.csv', index_col='match_id')\n",
    "X_test  = pd.read_csv('input/features_test.csv', index_col='match_id')\n",
    "\n",
    "del_columns = [x for x in set(X_train.columns) - set(X_test.columns)] # target и дополнительные столбцы результата\n",
    "y_train_df = X_train['radiant_win']\n",
    "y_train = np.array(y_train_df)\n",
    "X_train.drop(del_columns, axis=1, inplace=True)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "test_size  = X_test.shape[0]\n",
    "data = pd.concat((X_train, X_test)) # объединим выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем файлы с дополнительными данными, включая таблицу с характеристиками героев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroes = pd.read_csv('input/dictionaries/heroes.csv', index_col='id')\n",
    "abilities = pd.read_csv('input/dictionaries/abilities.csv', index_col='id')\n",
    "items = pd.read_csv('input/dictionaries/items.csv', index_col='id')\n",
    "\n",
    "heroes_database = pd.read_csv('input/dictionaries/heroes_db.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлечение мешков слов для предметов и умений из JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем исправленный скрипт для извлечения признаков основного датасета. Получим таблици, в которых строки соответствуют матчам, а столбцы предметам и умениям (первая половина столбцов одной команде, вторая половина другой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python2 extract_items.py input/matches.jsonlines.bz2      input/items_train.csv\n",
    "!python2 extract_items.py input/matches_test.jsonlines.bz2 input/items_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python2 extract_abilities.py input/matches.jsonlines.bz2      input/abilities_train.csv\n",
    "!python2 extract_abilities.py input/matches_test.jsonlines.bz2 input/abilities_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приготовление метапризнаков по мешкам слов из героев, предметов и умений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модели отдельно на каждом из мешков и предскажем вероятность победы radiant.\n",
    "\n",
    "Начнём с героев. Используем 4 модели для создания метапризнаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroes_match = np.zeros((data.shape[0], heroes.shape[0]))\n",
    "\n",
    "for i, match_id in enumerate(data.index):\n",
    "    for p in range(5):\n",
    "        heroes_match[i, data.ix[match_id, 'r%d_hero' % (p + 1)] - 1] = +1\n",
    "        heroes_match[i, data.ix[match_id, 'd%d_hero' % (p + 1)] - 1] = -1\n",
    "\n",
    "heroes_match_train = heroes_match[:train_size]\n",
    "heroes_match_test  = heroes_match[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = [XGBClassifier(max_depth=4, n_estimators=1400, learning_rate=0.04, min_child_weight=7, colsample_bytree=0.4,\n",
    "                      subsample=0.7, reg_alpha=1, reg_lambda=30, seed=789),\n",
    "        KNeighborsClassifier(n_neighbors=10, p=2, weights='distance', n_jobs=-1),\n",
    "        LogisticRegression(C=0.025, penalty='l2'),\n",
    "        RandomForestClassifier(criterion='gini', n_estimators=1000, max_depth=12, max_features='auto', min_samples_split=2,\n",
    "                               min_samples_leaf=7 ,random_state=101, n_jobs=-1)]\n",
    "\n",
    "heroes_meta_train = np.zeros((train_size, len(clfs)))\n",
    "heroes_meta_test  = np.zeros((test_size,  len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print('Clf', j)\n",
    "    dataset_meta_test_j = np.zeros((test_size, len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print('Fold', i)\n",
    "        X_tr = heroes_match_train[train]\n",
    "        X_ts = heroes_match_train[test]\n",
    "        y_tr = y_train[train]\n",
    "        y_ts = y_train[test]\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        heroes_meta_train[test, j] = clf.predict_proba(X_ts)[:, 1]\n",
    "        dataset_meta_test_j[:, i] = clf.predict_proba(heroes_match_test)[:, 1]\n",
    "    heroes_meta_test[:, j] = dataset_meta_test_j.mean(1)\n",
    "\n",
    "# np.save('input/heroes_meta_train.npy', heroes_meta_train)\n",
    "# np.save('input/heroes_meta_test.npy',  heroes_meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# heroes_meta_train = np.load('input/heroes_meta_train.npy')\n",
    "# heroes_meta_test  = np.load('input/heroes_meta_test.npy')\n",
    "\n",
    "heroes_meta_train = pd.DataFrame(heroes_meta_train, columns=['heroes_xgb', 'heroes_knn', 'heroes_lr', 'heroes_rf'], index=X_train.index)\n",
    "heroes_meta_test  = pd.DataFrame(heroes_meta_test,  columns=['heroes_xgb', 'heroes_knn', 'heroes_lr', 'heroes_rf'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдём к предметам. Используем созданный ранее мешок слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items_train = pd.read_csv('input/items_train.csv', index_col='match_id')\n",
    "items_test  = pd.read_csv('input/items_test.csv',  index_col='match_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим те предметы, которые ни разу не встречаются в обучающей выборке (среди героев таковых всего несколько штук, а вот предметов заметно больше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sum = items_train.sum(axis=0)\n",
    "\n",
    "for i, col in enumerate(items_train.columns[:254]):\n",
    "    if train_sum[i] == 0 and train_sum[i + 254] == 0:\n",
    "        items_train.drop(col, axis=1, inplace=True)\n",
    "        items_train.drop(col.replace('radiant', 'dire'), axis=1, inplace=True)\n",
    "\n",
    "        items_test.drop(col, axis=1, inplace=True)\n",
    "        items_test.drop(col.replace('radiant', 'dire'), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отмасштабируем и используем 4 модели для изготовления метапризнаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "items_train = scaler.fit_transform(items_train)\n",
    "items_test  = scaler.transform(items_test)\n",
    "\n",
    "skf = list(KFold(y_train.shape[0], 5, shuffle=True, random_state=909))\n",
    "\n",
    "clfs = [XGBClassifier(max_depth=4, n_estimators=1500, learning_rate=0.02, min_child_weight=5, colsample_bytree=0.3,\n",
    "                      subsample=0.9, reg_alpha=5, reg_lambda=1, seed=4),\n",
    "        KNeighborsClassifier(n_neighbors=10, p=2, weights='distance', n_jobs=-1),\n",
    "        LogisticRegression(C=0.005, penalty='l2'),\n",
    "        RandomForestClassifier(criterion='entropy', n_estimators=1000, max_depth=8, max_features='auto', min_samples_split=3,\n",
    "                               min_samples_leaf=4, random_state=51, n_jobs=-1)]\n",
    "\n",
    "items_meta_train = np.zeros((train_size, len(clfs)))\n",
    "items_meta_test  = np.zeros((test_size,  len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print('Clf', j)\n",
    "    dataset_meta_test_j = np.zeros((test_size, len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print('Fold', i)\n",
    "        X_tr = items_train[train]\n",
    "        X_ts = items_train[test]\n",
    "        y_tr = y_train[train]\n",
    "        y_ts = y_train[test]\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        items_meta_train[test, j] = clf.predict_proba(X_ts)[:, 1]\n",
    "        dataset_meta_test_j[:, i] = clf.predict_proba(items_test)[:, 1]\n",
    "    items_meta_test[:, j] = dataset_meta_test_j.mean(1)\n",
    "\n",
    "# np.save('input/items_meta_train.npy', items_meta_train)\n",
    "# np.save('input/items_meta_test.npy',  items_meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# items_meta_train = np.load('input/items_meta_train.npy')\n",
    "# items_meta_test  = np.load('input/items_meta_test.npy')\n",
    "\n",
    "items_meta_train = pd.DataFrame(items_meta_train, columns=['items_xgb', 'items_knn', 'items_lr', 'items_rf'], index=X_train.index)\n",
    "items_meta_test  = pd.DataFrame(items_meta_test,  columns=['items_xgb', 'items_knn', 'items_lr', 'items_rf'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь умения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abilities_train = pd.read_csv('input/abilities_train.csv', index_col='match_id')\n",
    "abilities_test  = pd.read_csv('input/abilities_test.csv',  index_col='match_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова удалим нулевые столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sum = abilities_train.sum(axis=0)\n",
    "\n",
    "for i, col in enumerate(abilities_train.columns[:568]):\n",
    "    if train_sum[i] == 0 and train_sum[i + 568] == 0:\n",
    "        abilities_train.drop(col, axis=1, inplace=True)\n",
    "        abilities_train.drop(col.replace('radiant', 'dire'), axis=1, inplace=True)\n",
    "\n",
    "        abilities_test.drop(col, axis=1, inplace=True)\n",
    "        abilities_test.drop(col.replace('radiant', 'dire'), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отмасштабируем и используем 5 моделей для метапризнаков (ранее не использовал ET, потому что забыл о нём). Это будет очень долго работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "abilities_train = scaler.fit_transform(abilities_train)\n",
    "abilities_test  = scaler.transform(abilities_test)\n",
    "\n",
    "skf = list(KFold(y_train.shape[0], 5, shuffle=True, random_state=65))\n",
    "\n",
    "clfs = [XGBClassifier(max_depth=6, n_estimators=700, learning_rate=0.02, seed=178),\n",
    "        LogisticRegression(C=1.0, penalty='l2'),\n",
    "        KNeighborsClassifier(n_neighbors=10, p=2, weights='distance', n_jobs=1),\n",
    "        RandomForestClassifier(criterion='entropy', n_estimators=500, max_depth=10, max_features='sqrt', min_samples_split=2,\n",
    "                               min_samples_leaf=1 ,random_state=90, n_jobs=1),\n",
    "        ExtraTreesClassifier(criterion='gini', max_depth=9, n_estimators=500,\n",
    "                             max_features='sqrt',random_state=45)]\n",
    "\n",
    "\n",
    "abilities_meta_train = np.zeros((train_size, len(clfs)))\n",
    "abilities_meta_test  = np.zeros((test_size,  len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print('Clf', j)\n",
    "    dataset_meta_test_j = np.zeros((test_size, len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print('Fold', i)\n",
    "        X_tr = abilities_train[train]\n",
    "        X_ts = abilities_train[test]\n",
    "        y_tr = y_train[train]\n",
    "        y_ts = y_train[test]\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        abilities_meta_train[test, j] = clf.predict_proba(X_ts)[:, 1]\n",
    "        dataset_meta_test_j[:, i] = clf.predict_proba(abilities_test)[:, 1]\n",
    "    abilities_meta_test[:, j] = dataset_meta_test_j.mean(1)\n",
    "\n",
    "# np.save('input/abilities_meta_train.npy', abilities_meta_train)\n",
    "# np.save('input/abilities_meta_test.npy',  abilities_meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# abilities_meta_train = np.load('input/abilities_meta_train.npy')\n",
    "# abilities_meta_test  = np.load('input/abilities_meta_test.npy')\n",
    "\n",
    "abilities_meta_train = pd.DataFrame(abilities_meta_train, columns=['abilities_xgb', 'abilities_knn', 'abilities_lr', 'abilities_rf', 'abilities_et'], index=X_train.index)\n",
    "abilities_meta_test  = pd.DataFrame(abilities_meta_test,  columns=['abilities_xgb', 'abilities_knn', 'abilities_lr', 'abilities_rf', 'abilities_et'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Первая модель - Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим первую модель (на самом деле лучший из двух результатов на отдельных моделях).\n",
    "\n",
    "Не будем использовать метапризнак, основанный на умениях (не был готов к тому моменту). Также не будем использовать характеристики героев (тоже не было готово)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые для создания новых и удаления старых признаков функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_differences(data, cols):\n",
    "    for col in cols:\n",
    "        data[col + '_dif'] = np.sum([data['r%d_' % (i) + col] for i in range(1, 6)], axis=0) - \\\n",
    "                             np.sum([data['d%d_' % (i) + col] for i in range(1, 6)], axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_ratios(data, cols):\n",
    "    for col in cols:\n",
    "        data[col + '_rat'] = np.sum([data['r%d_' % (i) + col] for i in range(1, 6)], axis=0) / \\\n",
    "                             np.sum([data['d%d_' % (i) + col] for i in range(1, 6)], axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_max_differences(data, cols):\n",
    "    for col in cols:\n",
    "        data[col + '_maxdif'] = np.max([data['r%d_' % (i) + col] for i in range(1, 6)], axis=0) - \\\n",
    "                                np.max([data['d%d_' % (i) + col] for i in range(1, 6)], axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_min_differences(data, cols):\n",
    "    for col in cols:\n",
    "        data[col + '_mindif'] = np.min([data['r%d_' % (i) + col] for i in range(1, 6)], axis=0) - \\\n",
    "                                np.min([data['d%d_' % (i) + col] for i in range(1, 6)], axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_std_differences(data, cols):\n",
    "    for col in cols:\n",
    "        data[col + '_stddif'] = np.std([data['r%d_' % (i) + col] for i in range(1, 6)], axis=0) - \\\n",
    "                                np.std([data['d%d_' % (i) + col] for i in range(1, 6)], axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def del_individs(data, cols):\n",
    "    for col in cols:\n",
    "        data.drop(['%c%d_' % (c, i) + col for i in range(1, 6) for c in ['r', 'd']], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим новые признаки, основанные на разностях и отношеиях сумм характеристик героев команды, разностях между максимальными и минимальными значениями характеристик, разностях среднеквадратичных отклонений признаков команд. Добавим отдельно максимумы по опытам каждой из команд. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_differences     = ['level', 'xp', 'gold', 'lh', 'items']\n",
    "cols_ratios          = ['xp', 'gold']\n",
    "cols_max_differences = ['level', 'xp', 'gold', 'lh', 'items']\n",
    "cols_min_differences = ['xp', 'gold', 'lh']\n",
    "cols_std_differences = ['xp', 'gold', 'lh']\n",
    "\n",
    "data = add_differences(data, cols_differences)\n",
    "data = add_ratios(data, cols_ratios)\n",
    "data = add_max_differences(data, cols_max_differences)\n",
    "data = add_min_differences(data, cols_min_differences)\n",
    "data = add_std_differences(data, cols_std_differences)\n",
    "\n",
    "data['r_xp_max'] = np.max([data['r%d_xp' % (i)] for i in range(1, 6)], axis=0)\n",
    "data['d_xp_max'] = np.max([data['d%d_xp' % (i)] for i in range(1, 6)], axis=0)\n",
    "\n",
    "data['bottle_time_dif'] = data.radiant_bottle_time - data.dire_bottle_time\n",
    "\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим почти все индивидуальные признаки и некоторые из остальных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_del_individs = ['hero', 'level', 'kills', 'deaths', 'items', 'lh', 'gold']\n",
    "cols_del          = ['first_blood_team', 'radiant_ward_sentry_count', 'dire_ward_sentry_count',\n",
    "                     'first_blood_player1', 'first_blood_player2', 'radiant_ward_observer_count',\n",
    "                     'dire_ward_observer_count', 'lobby_type']\n",
    "\n",
    "data = del_individs(data, cols_del_individs)\n",
    "data.drop(cols_del, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратно разделим на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data.iloc[:train_size]\n",
    "X_test  = data.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию для генерации датасета для Vowpal Wabbit по базовым признакам, мешкам из героев, предметов и умений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(data, heroes, items, abilities, name, is_train, y_train=None):\n",
    "\n",
    "    half_items = items.shape[1] // 2\n",
    "    half_abilities = abilities.shape[1] // 2\n",
    "\n",
    "    with open(name, 'w') as fout:\n",
    "\n",
    "        for match_id in data.index:\n",
    "\n",
    "            row           = data.ix[match_id]\n",
    "            row_heroes    = heroes.ix[match_id]\n",
    "            row_items     = items.ix[match_id]\n",
    "            row_abilities = abilities.ix[match_id]\n",
    "\n",
    "            if is_train:\n",
    "                target = 1 if y_train.ix[match_id] == 1 else -1\n",
    "            else:\n",
    "                target = 1\n",
    "\n",
    "            fout.write(str(target) +\n",
    "                       ' |features ' + ' '.join('{0}:{1}'.format(i, j) for i, j in zip(data.columns, row)) +\n",
    "                       ' |heroes '  + ' '.join('{0}_{1}'.format('r' if i == 1 else 'd', heroes.columns[j]) for j, i in enumerate(row_heroes) if i != 0) +\n",
    "                       ' |items ' + ' '.join('{0}:{1}'.format(items.columns[i], j) for i, j in enumerate(row_items) if j != 0) +\n",
    "                       ' |abilities ' + ' '.join('{0}:{1}'.format(abilities.columns[i], j) for i, j in enumerate(row_abilities) if j != 0) +\n",
    "                       '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отмасштабируем базовые признаки и разделим мешок слов по героям на две части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test  = pd.DataFrame(scaler.transform(X_test),      columns=X_test.columns,  index=X_test.index)\n",
    "\n",
    "heroes_match_train = heroes_match.iloc[:train_size]\n",
    "heroes_match_test  = heroes_match.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим признаки для VW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_features(X_train_vw, heroes_match_train, items_train, abilities_train, 'input/train.vw', True, y_train)\n",
    "make_features(X_test_vw,  heroes_match_test,  items_test,  abilities_test,  'input/test.vw',  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучимся и сделаем предсказание (параметры оказались не самыми оптимальным: при уменьшении темпа обучения качество повышается)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system('vw -d input/train.vw -c -k -f model.vw --passes 200 -l 0.09 --power_t 0.36 --initial_t 0.1 -b 26 --loss_function logistic --quiet')\n",
    "os.system('vw -d input/test.vw -i model.vw -t -p output/predictions_tmp.txt --quiet')\n",
    "\n",
    "preds1 = pd.read_csv('output/predictions_tmp.txt', header=None).iloc[:, 0].values\n",
    "preds1 = pd.DataFrame(preds1, columns=['radiant_win'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Private LB: 0.76413 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлечение информации из характеристик героев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем заново базовые признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('input/features.csv', index_col='match_id')\n",
    "X_test  = pd.read_csv('input/features_test.csv', index_col='match_id')\n",
    "\n",
    "del_columns = [x for x in set(X_train.columns) - set(X_test.columns)] # target и дополнительные столбцы результата\n",
    "y_train_df = X_train['radiant_win']\n",
    "y_train = np.array(y_train_df)\n",
    "X_train.drop(del_columns, axis=1, inplace=True)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "test_size  = X_test.shape[0]\n",
    "data = pd.concat((X_train, X_test)) # объединим выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечём характеристики каждого из героев из базы данных характеристик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in ['r', 'd']:\n",
    "    for i in range(1, 6):\n",
    "        data['%c%d_strength'      % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'STR'])\n",
    "        data['%c%d_strength+'     % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'STR+'])\n",
    "        data['%c%d_intelligence'  % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'INT'])\n",
    "        data['%c%d_intelligence+' % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'INT+'])\n",
    "        data['%c%d_agility'       % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'AGI'])\n",
    "        data['%c%d_agility+'      % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'AGI+'])\n",
    "        data['%c%d_attrs'         % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'T'])\n",
    "        data['%c%d_attrs+'        % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'T+'])\n",
    "        data['%c%d_speed'         % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'MOV'])\n",
    "        data['%c%d_damage'        % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, '(MAX)'] + heroes_database.ix[x, '(MIN)'])\n",
    "        data['%c%d_ranged'        % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'RNG'])\n",
    "        data['%c%d_attacktime'    % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'BAT'])\n",
    "\n",
    "for c in ['r', 'd']:\n",
    "    for i in range(1, 6):\n",
    "        data['%c%d_strength'     % (c, i)] += data['%c%d_strength+'     % (c, i)] * data['%c%d_level' % (c, i)]\n",
    "        data['%c%d_intelligence' % (c, i)] += data['%c%d_intelligence+' % (c, i)] * data['%c%d_level' % (c, i)]\n",
    "        data['%c%d_agility'      % (c, i)] += data['%c%d_agility+'      % (c, i)] * data['%c%d_level' % (c, i)]\n",
    "        data['%c%d_attrs'        % (c, i)] += data['%c%d_attrs+'        % (c, i)] * data['%c%d_level' % (c, i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова аггрегируем характеристики команд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_differences     = ['level', 'xp', 'gold', 'lh', 'items', 'strength', 'intelligence', 'agility', 'attrs', 'speed', 'damage', 'ranged', 'attacktime']\n",
    "cols_ratios          = ['xp', 'gold', 'strength', 'intelligence', 'agility']\n",
    "cols_max_differences = ['level', 'xp', 'gold', 'lh', 'items', 'strength', 'intelligence', 'agility', 'attrs', 'speed', 'damage', 'ranged', 'attacktime']\n",
    "cols_min_differences = ['xp', 'gold', 'lh']\n",
    "cols_std_differences = ['xp', 'gold', 'lh', 'strength', 'intelligence', 'agility']\n",
    "\n",
    "data = add_differences(data, cols_differences)\n",
    "data = add_ratios(data, cols_ratios)\n",
    "data = add_max_differences(data, cols_max_differences)\n",
    "data = add_min_differences(data, cols_min_differences)\n",
    "data = add_std_differences(data, cols_std_differences)\n",
    "\n",
    "data['r_xp_max'] = np.max([data['r%d_xp' % (i)] for i in range(1, 6)], axis=0)\n",
    "data['d_xp_max'] = np.max([data['d%d_xp' % (i)] for i in range(1, 6)], axis=0)\n",
    "\n",
    "data['bottle_time_dif'] = data.radiant_bottle_time - data.dire_bottle_time\n",
    "\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем, сколько героем, каждой из ролей встречается в командах. Возьмём разность показателей между командами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roles = ['Carry', 'Disabler', 'Lane_support', 'Initiator', 'Jungler', 'Support', 'Durable', 'Pusher', 'Nuker', 'Escape']\n",
    "match_roles_table = np.zeros((data.shape[0], len(roles)))\n",
    "\n",
    "for j, match_id in enumerate(data.index):\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        match_roles_table[j] += np.array(heroes_database.ix[data.ix[match_id, 'r%d_hero' % (i)], roles], dtype=int)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        match_roles_table[j] -= np.array(heroes_database.ix[data.ix[match_id, 'd%d_hero' % (i)], roles], dtype=int)\n",
    "\n",
    "match_roles_table = pd.DataFrame(match_roles_table, columns=roles, index=data.index)\n",
    "match_roles_table.drop('Lane_support', axis=1, inplace=True) # не встречается\n",
    "data = pd.concat((data, match_roles_table), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим ненужное (столько, пока у логистической регрессии из scikit-learn повышается результат)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_del_individs = ['damage', 'items', 'level', 'kills', 'deaths', 'intelligence+', 'strength+', 'agility+' , 'attacktime', 'speed', 'ranged']\n",
    "\n",
    "cols_del          = ['first_blood_team', 'radiant_ward_sentry_count', 'dire_ward_sentry_count',\n",
    "                     'radiant_ward_observer_count', 'dire_ward_observer_count']\n",
    "\n",
    "data = del_individs(data, cols_del_individs)\n",
    "data.drop(cols_del, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим и отмасштабируем (float32 для theano)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data.iloc[:train_size]\n",
    "X_test  = data.iloc[train_size:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = np.array(scaler.fit_transform(X_train), dtype=np.float32)\n",
    "X_test  = np.array(scaler.transform(X_test),      dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пригодится для нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [('input', InputLayer),\n",
    "          ('dense0', DenseLayer),\n",
    "          ('dropout0', DropoutLayer),\n",
    "          ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем 5 моделей для генерации метапризнаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = list(KFold(y_train.shape[0], 5, shuffle=True, random_state=8))\n",
    "\n",
    "clfs = [NeuralNet(layers=layers, input_shape=(None, X_train.shape[1]),\n",
    "                  dense0_num_units=80, dropout0_p=0.5,\n",
    "                  output_num_units=2, output_nonlinearity=softmax,\n",
    "                  update=adagrad, update_learning_rate=0.003,\n",
    "                  train_split=TrainSplit(eval_size=0.2),\n",
    "                  verbose=0, max_epochs=100),\n",
    "        LogisticRegression(C=0.01, penalty='l2'),\n",
    "        RandomForestClassifier(criterion='entropy', n_estimators=700, max_depth=7,\n",
    "                               random_state=500, n_jobs=-1),\n",
    "        ExtraTreesClassifier(criterion='gini', max_depth=12, n_estimators=700,\n",
    "                             random_state=8010, n_jobs=-1),\n",
    "        XGBClassifier(max_depth=4, n_estimators=700, learning_rate=0.015,\n",
    "                      min_child_weight=5, seed=114)]\n",
    "\n",
    "\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "\n",
    "meta_train = np.zeros((train_size, len(clfs)))\n",
    "meta_test  = np.zeros((test_size,  len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print('Clf', j)\n",
    "    dataset_meta_test_j = np.zeros((test_size, len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print('Fold', i)\n",
    "        X_tr = X_train[train]\n",
    "        X_ts = X_train[test]\n",
    "        y_tr = y_train[train]\n",
    "        y_ts = y_train[test]\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        meta_train[test, j] = clf.predict_proba(X_ts)[:, 1]\n",
    "        dataset_meta_test_j[:, i] = clf.predict_proba(X_test)[:, 1]\n",
    "    meta_test[:, j] = dataset_meta_test_j.mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем метапризнаки с помощью Vowpal Wabbit с мешками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_vw = data.iloc[:train_size]\n",
    "X_test_vw  = data.iloc[train_size:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_vw = pd.DataFrame(scaler.fit_transform(X_train_vw), columns=X_train_vw.columns, index=X_train_vw.index)\n",
    "X_test_vw  = pd.DataFrame(scaler.transform(X_test_vw),      columns=X_test_vw.columns,  index=X_test_vw.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим 2 VW: на всех признаках и мешках и только на мешках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_vw_train = np.zeros((train_size, 2))\n",
    "meta_vw_test  = np.zeros((test_size,  2))\n",
    "\n",
    "dataset_meta_test_j1 = np.zeros((test_size, len(skf)))\n",
    "dataset_meta_test_j2 = np.zeros((test_size, len(skf)))\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print('Fold', i)\n",
    "\n",
    "    make_features(X_train_vw, heroes_match_train, items_train, abilities_train, 'input/train.vw', True, y_train)\n",
    "    make_features(X_test_vw,  heroes_match_test,  items_test,  abilities_test, 'input/test.vw',  False)\n",
    "\n",
    "    make_features(X_train_vw.iloc[train], heroes_match_train.iloc[train], items_train.iloc[train], abilities_train.iloc[train], 'input/train_cv.vw', True, y_train[train])\n",
    "    make_features(X_train_vw.iloc[test],  heroes_match_train.iloc[test],  items_train.iloc[test],  abilities_train.iloc[test],  'input/test_cv.vw',  False)\n",
    "\n",
    "    # first cv\n",
    "    os.system('vw -d input/train_cv.vw -c -k -f model.vw --passes 200 -l 0.05 --power_t 0.5 --initial_t 0.0 -b 26 --loss_function logistic --quiet')\n",
    "    os.system('vw -d input/test_cv.vw -i model.vw -t -p output/predictions_tmp.txt --quiet')\n",
    "\n",
    "    meta_vw_train[test, 0] = pd.read_csv('output/predictions_tmp.txt', header=None).iloc[:, 0].values\n",
    "\n",
    "    # first test\n",
    "    os.system('vw -d input/train.vw -c -k -f model.vw --passes 200 -l 0.05 --power_t 0.5 --initial_t 0.0 -b 26 --loss_function logistic --quiet')\n",
    "    os.system('vw -d input/test.vw -i model.vw -t -p output/predictions_tmp.txt --quiet')\n",
    "\n",
    "    dataset_meta_test_j1[:, i] = pd.read_csv('output/predictions_tmp.txt', header=None).iloc[:, 0].values\n",
    "\n",
    "    # second cv\n",
    "    os.system('vw -d input/train_cv.vw -c -k -f model.vw --passes 200 -l 0.2 --power_t 0.43 --initial_t 0.0 -b 26 --loss_function logistic --quiet --ignore f')\n",
    "    os.system('vw -d input/test_cv.vw -i model.vw -t -p output/predictions_tmp.txt --quiet')\n",
    "\n",
    "    meta_vw_train[test, 1] = pd.read_csv('output/predictions_tmp.txt', header=None).iloc[:, 0].values\n",
    "\n",
    "    # second test\n",
    "    os.system('vw -d input/train.vw -c -k -f model.vw --passes 200 -l 0.2 --power_t 0.43 --initial_t 0.0 -b 26 --loss_function logistic --quiet --ignore f')\n",
    "    os.system('vw -d input/test.vw -i model.vw -t -p output/predictions_tmp.txt --quiet')\n",
    "\n",
    "    dataset_meta_test_j2[:, i] = pd.read_csv('output/predictions_tmp.txt', header=None).iloc[:, 0].values\n",
    "\n",
    "\n",
    "meta_vw_test[:, 0] = dataset_meta_test_j1.mean(1)\n",
    "meta_vw_test[:, 1] = dataset_meta_test_j2.mean(1)\n",
    "\n",
    "meta_train = np.hstack((meta_train, meta_vw_train))\n",
    "meta_test  = np.hstack((meta_test,  meta_vw_test))\n",
    "\n",
    "# np.save('input/X_meta_train.npy', meta_train)\n",
    "# np.save('input/X_meta_test.npy',  meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta_train = np.load('input/X_meta_train.npy')\n",
    "# meta_test  = np.load('input/X_meta_test.npy')\n",
    "\n",
    "meta_train = pd.DataFrame(meta_train, columns=['nn_meta', 'lr_meta', 'rf_meta', 'et_meta', 'xgb_meta', 'vw1_meta', 'vw2_meta'], index=X_train_vw.index)\n",
    "meta_test  = pd.DataFrame(meta_test,  columns=['nn_meta', 'lr_meta', 'rf_meta', 'et_meta', 'xgb_meta', 'vw1_meta', 'vw2_meta'], index=X_test_vw.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метапризнак по синергии и антисинергии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('input/features.csv', index_col='match_id')\n",
    "X_test  = pd.read_csv('input/features_test.csv', index_col='match_id')\n",
    "\n",
    "del_columns = [x for x in set(X_train.columns) - set(X_test.columns)] # target и дополнительные столбцы результата\n",
    "y_train_df = X_train['radiant_win']\n",
    "y_train = np.array(y_train_df)\n",
    "X_train.drop(del_columns, axis=1, inplace=True)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "test_size  = X_test.shape[0]\n",
    "data = pd.concat((X_train, X_test)) # объединим выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подсчёта синергии по тренировочкой и тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_synergy(train, test, y_train):\n",
    "\n",
    "    heroes_count = 113\n",
    "    train_size   = train.shape[0]\n",
    "    synergy     = np.zeros((heroes_count, heroes_count)) \n",
    "    antisynergy = np.zeros((heroes_count, heroes_count)) \n",
    "    matchcounts = np.zeros((heroes_count, heroes_count)) \n",
    "    matchcounta = np.zeros((heroes_count, heroes_count)) \n",
    "\n",
    "    # считаем статистику\n",
    "    for match_counter, match_id in enumerate(train.index):\n",
    "        \n",
    "        winteam = 'r' if y_train.ix[match_id] == 1 else 'd'\n",
    "        looseteam = 'd' if winteam == 'r' else 'r'\n",
    "        pind     = [0] * 5 \n",
    "        antipind = [0] * 5 \n",
    "        \n",
    "        for i in range(5):\n",
    "            pind[i] = train.ix[match_id, winteam + '%d_hero' % (i + 1)] - 1\n",
    "        for i in range(5):\n",
    "            antipind[i] = train.ix[match_id, looseteam + '%d_hero' % (i + 1)] - 1\n",
    "        \n",
    "        for i in range(5):\n",
    "            for j in range(i+1,5):\n",
    "                synergy[pind[i], pind[j]] += 1\n",
    "                synergy[pind[j], pind[i]] += 1\n",
    "        \n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                matchcounts[pind[i], pind[j]] += 1 \n",
    "                matchcounts[antipind[i], antipind[j]] += 1 \n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                antisynergy[pind[i], antipind[j]] += 1\n",
    "                matchcounta[pind[i], antipind[j]] += 1\n",
    "                matchcounta[antipind[j], pind[i]] += 1\n",
    "\n",
    "    synergyrate     = np.zeros((heroes_count, heroes_count))\n",
    "    antisynergyrate = np.zeros((heroes_count, heroes_count))\n",
    "    \n",
    "    # нормализуем\n",
    "    for i in range(heroes_count):\n",
    "        for j in range(heroes_count):\n",
    "            if matchcounts[i, j] != 0:\n",
    "                synergyrate[i,j] = synergy[i, j] / matchcounts[i, j]\n",
    "            else:\n",
    "                synergyrate[i, j] = 0.5\n",
    "            if matchcounta[i, j] != 0:\n",
    "                antisynergyrate[i, j] = antisynergy[i, j] / matchcounta[i, j]\n",
    "            else:\n",
    "                antisynergyrate[i, j] = 0.5\n",
    "\n",
    "    syn     = np.zeros(len(test))\n",
    "    antisyn = np.zeros(len(test))\n",
    "    \n",
    "    # подсчитываем для тестовой выборки\n",
    "    for match_counter, match_id in enumerate(test.index):\n",
    "        rind = [0] * 5 \n",
    "        dind = [0] * 5 \n",
    "        \n",
    "        for i in range(5):\n",
    "            rind[i] = test.ix[match_id, 'r%d_hero' % (i + 1)] - 1\n",
    "        for i in range(5):\n",
    "            dind[i] = test.ix[match_id, 'd%d_hero' % (i + 1)] - 1\n",
    "        \n",
    "        for i in range(5):\n",
    "            for j in range(i + 1, 5):\n",
    "                syn[match_counter] += synergyrate[rind[i], rind[j]]\n",
    "        \n",
    "        for i in range(5):\n",
    "            for j in range(i + 1, 5):\n",
    "                syn[match_counter] -= synergyrate[dind[i], dind[j]]\n",
    "        \n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                antisyn[match_counter] += antisynergyrate[rind[i], dind[j]]\n",
    "\n",
    "    return syn, antisyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = list(KFold(y_train.shape[0], 10, random_state=7))\n",
    "\n",
    "synergy_train      = np.zeros(train_size)\n",
    "synergy_test       = np.zeros(test_size)\n",
    "antisynergy_train  = np.zeros(train_size)\n",
    "antisynergy_test   = np.zeros(test_size)\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print('Fold', i)\n",
    "    X_tr  = X_train.iloc[train]\n",
    "    y_tr  = y_train.iloc[train]\n",
    "    X_ts  = X_train.iloc[test]\n",
    "    y_ts  = y_train.iloc[test]\n",
    "\n",
    "    synergy_train[test], antisynergy_train[test] = add_synergy(X_tr, X_ts, y_tr)\n",
    "\n",
    "synergy_test, antisynergy_test = add_synergy(X_train, X_test, y_train)\n",
    "\n",
    "syn_train = np.hstack((synergy_train.reshape((-1, 1)), antisynergy_train.reshape((-1, 1))))\n",
    "syn_test  = np.hstack((synergy_test.reshape((-1, 1)),  antisynergy_test.reshape((-1, 1))))\n",
    "\n",
    "# np.save('input/meta_synergy_train.npy', syn_train)\n",
    "# np.save('input/meta_synergy_test.npy',  syn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# syn_train = np.load('input/meta_synergy_train.npy')\n",
    "# syn_test  = np.load('input/meta_synergy_test.npy')\n",
    "\n",
    "syn_train = pd.DataFrame(syn_train, columns=['synergy', 'antisynergy'], index=X_train.index)\n",
    "syn_test  = pd.DataFrame(syn_test,  columns=['synergy', 'antisynergy'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вторая модель - Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать все метапризнаки: по мешкам, по другим алгоритмам на обычных признаках, по синергии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_meta_train = pd.concat((heroes_meta_train, items_meta_train, abilities_meta_train), axis=1)\n",
    "bag_meta_test  = pd.concat((heroes_meta_test,  items_meta_test,  abilities_meta_test),  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова загрузим и обработаем признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('input/features.csv', index_col='match_id')\n",
    "X_test  = pd.read_csv('input/features_test.csv', index_col='match_id')\n",
    "\n",
    "del_columns = [x for x in set(X_train.columns) - set(X_test.columns)] # target и дополнительные столбцы результата\n",
    "y_train_df = X_train['radiant_win']\n",
    "y_train = np.array(y_train_df)\n",
    "X_train.drop(del_columns, axis=1, inplace=True)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "test_size  = X_test.shape[0]\n",
    "data = pd.concat((X_train, X_test)) # объединим выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in ['r', 'd']:\n",
    "    for i in range(1, 6):\n",
    "        data['%c%d_strength'      % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'STR'])\n",
    "        data['%c%d_strength+'     % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'STR+'])\n",
    "        data['%c%d_intelligence'  % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'INT'])\n",
    "        data['%c%d_intelligence+' % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'INT+'])\n",
    "        data['%c%d_agility'       % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'AGI'])\n",
    "        data['%c%d_agility+'      % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'AGI+'])\n",
    "        data['%c%d_attrs'         % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'T'])\n",
    "        data['%c%d_attrs+'        % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'T+'])\n",
    "        data['%c%d_speed'         % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'MOV'])\n",
    "        data['%c%d_damage'        % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, '(MAX)'] + heroes_database.ix[x, '(MIN)'])\n",
    "        data['%c%d_ranged'        % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'RNG'])\n",
    "        data['%c%d_attacktime'    % (c, i)] = data['%c%d_hero' % (c, i)].apply(lambda x: heroes_database.ix[x, 'BAT'])\n",
    "\n",
    "for c in ['r', 'd']:\n",
    "    for i in range(1, 6):\n",
    "        data['%c%d_strength'     % (c, i)] += data['%c%d_strength+'     % (c, i)] * data['%c%d_level' % (c, i)]\n",
    "        data['%c%d_intelligence' % (c, i)] += data['%c%d_intelligence+' % (c, i)] * data['%c%d_level' % (c, i)]\n",
    "        data['%c%d_agility'      % (c, i)] += data['%c%d_agility+'      % (c, i)] * data['%c%d_level' % (c, i)]\n",
    "        data['%c%d_attrs'        % (c, i)] += data['%c%d_attrs+'        % (c, i)] * data['%c%d_level' % (c, i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать несколько другие признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_differences     = ['level', 'xp', 'gold', 'lh', 'items', 'strength', 'intelligence', 'agility', 'attrs', 'speed', 'damage', 'ranged', 'attacktime']\n",
    "cols_ratios          = ['xp', 'gold', 'strength', 'intelligence', 'agility']\n",
    "cols_max_differences = ['level', 'xp', 'gold', 'lh', 'items', 'strength', 'intelligence', 'agility', 'attrs', 'speed', 'damage', 'ranged', 'attacktime']\n",
    "cols_min_differences = ['xp', 'gold', 'lh']\n",
    "cols_std_differences = ['xp', 'gold', 'lh', 'strength', 'intelligence', 'agility']\n",
    "\n",
    "data = add_differences(data, cols_differences)\n",
    "data = add_ratios(data, cols_ratios)\n",
    "data = add_max_differences(data, cols_max_differences)\n",
    "data = add_min_differences(data, cols_min_differences)\n",
    "data = add_std_differences(data, cols_std_differences)\n",
    "\n",
    "data['r_xp_max'] = np.max([data['r%d_xp' % (i)] for i in range(1, 6)], axis=0)\n",
    "data['d_xp_max'] = np.max([data['d%d_xp' % (i)] for i in range(1, 6)], axis=0)\n",
    "\n",
    "data['bottle_time_dif'] = data.radiant_bottle_time - data.dire_bottle_time\n",
    "\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_del_individs = ['ranged', 'kills', 'deaths', 'attacktime', 'speed', 'level', 'items', 'damage', 'strength+', 'hero', 'lh', 'xp',\n",
    "                     'gold', 'strength', 'attrs', 'intelligence']\n",
    "cols_del = ['first_blood_team', 'items_maxdif', 'lobby_type', 'first_blood_team', 'radiant_ward_sentry_count', \n",
    "            'dire_ward_sentry_count', 'first_blood_player1', 'level_maxdif', 'lh_mindif']\n",
    "\n",
    "data = del_individs(data, cols_del_individs)\n",
    "data.drop(cols_del, axis=1, inplace=True)\n",
    "\n",
    "X_train = data.iloc[:train_size]\n",
    "X_test  = data.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat((X_train, bag_meta_train), axis=1)\n",
    "X_test  = pd.concat((X_test,  bag_meta_test),  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test  = pd.DataFrame(scaler.transform(X_test),      columns=X_test.columns,  index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "meta_train = pd.DataFrame(scaler.fit_transform(meta_train), columns=meta_train.columns, index=X_train_vw.index)\n",
    "meta_test  = pd.DataFrame(scaler.transform(meta_test),      columns=meta_test.columns,  index=X_test_vw.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "syn_train = pd.DataFrame(scaler.fit_transform(syn_train), columns=syn_train.columns, index=X_train_vw.index)\n",
    "syn_test  = pd.DataFrame(scaler.transform(syn_test),      columns=syn_test.columns,  index=X_test_vw.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем изменённую функцию для генерации признаков для Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(data, heroes, items, abilities, meta, synergy, name, is_train, y_train=None):\n",
    "    \n",
    "    half_items = items.shape[1] // 2\n",
    "    half_abilities = abilities.shape[1] // 2\n",
    "    \n",
    "    with open(name, 'w') as fout:\n",
    "    \n",
    "        for match_id in log_progress(data.index, every=100):\n",
    "\n",
    "            row           = data.ix[match_id]\n",
    "            row_heroes    = heroes.ix[match_id]\n",
    "            row_items     = items.ix[match_id]\n",
    "            row_abilities = abilities.ix[match_id]\n",
    "            row_meta      = meta.ix[match_id]\n",
    "            row_synergy   = synergy.ix[match_id]\n",
    "\n",
    "            if is_train:\n",
    "                target = 1 if y_train.ix[match_id] == 1 else -1\n",
    "            else:\n",
    "                target = 1\n",
    "\n",
    "            fout.write(str(target) + \n",
    "                       ' |features ' + ' '.join('{0}:{1}'.format(i, j) for i, j in zip(data.columns, row)) +\n",
    "                       ' |heroes '  + ' '.join('{0}_{1}'.format('r' if i == 1 else 'd', heroes.columns[j]) for j, i in enumerate(row_heroes) if i != 0) +\n",
    "                       ' |items ' + ' '.join('{0}:{1}'.format(items.columns[i], j) for i, j in enumerate(row_items) if j != 0) + \n",
    "                       ' |abilities ' + ' '.join('{0}:{1}'.format(abilities.columns[i], j) for i, j in enumerate(row_abilities) if j != 0) + \n",
    "                       ' |meta ' + ' '.join('{0}:{1}'.format(meta.columns[i], j) for i, j in enumerate(row_meta) if j != 0) + \n",
    "                       ' |synergy ' + ' '.join('{0}:{1}'.format(synergy.columns[i], j) for i, j in enumerate(row_synergy) if j != 0) + \n",
    "                       '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_features(X_train_vw, heroes_match_train, items_train, abilities_train, meta_train, syn_train, 'input/train.vw', True, y_train)\n",
    "make_features(X_test_vw,  heroes_match_test,  items_test,  abilities_test,  meta_test,  syn_test,  'input/test.vw',  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим и предскажем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system('vw -d input/train.vw -c -k -f model.vw --passes 200 -l 0.5 --sgd -b 26 --loss_function logistic --quiet')\n",
    "os.system('vw -d input/test.vw -i model.vw -t -p output/predictions_tmp.txt --quiet')\n",
    "\n",
    "preds2 = pd.read_csv('output/predictions_tmp.txt', header=None).iloc[:, 0].values\n",
    "preds2 = pd.DataFrame(preds2, columns=['radiant_win'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Private LB: 0.76266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хуже предыдущего (вероятно, переобучение, но было трудно кросс-валидироваться с метапризнаками)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Смешаем решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициенты были выбраны наугад за час до конца контеста, когда оставался лишь один сабмит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds1 = (preds1 - preds1.min()) / (preds1.max() - preds1.min())\n",
    "preds2 = (preds2 - preds2.min()) / (preds2.max() - preds2.min())\n",
    "preds = preds1 * 0.8 + preds2 * 0.2\n",
    "preds.to_csv('output/ensemble.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Private LB: 0.76458"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
